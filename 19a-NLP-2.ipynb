{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** setup **********\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from re import sub\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "print(\"******** setup **********\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# get the newsgroup database\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=cats)\n",
    "\n",
    "# extract into dataframes\n",
    "text = pandas.DataFrame(newsgroups_train.data, columns=['text'])\n",
    "label = pandas.DataFrame(newsgroups_train.target, columns=['label'])['label'].apply(lambda x: cats[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahhh, remember the days of Yesterday?  When we were only \\n\\tgoing to pay $17 / month?\\n\\n\\tWhen only 1.2% of the population would pay extra taxes?\\n\\n\\tRemember when a few of us predicted that it wasn\\'t true?  :)\\n\\tRemember the Inaugural?   Dancing and Singing!  Liberation\\n\\tat last!  \\n\\n\\tWell, figure *this* out:\\n\\n\\t5% VAT, estimated to raise $60-100 Billion per year ( on CNN )\\n\\tWork it out, chum...\\n\\n\\t     $60,000,000,000  /  125,000,000 taxpayers = $480 / year\\n\\n        But, you exclaim, \" I\\'ll get FREE HEALTH CARE! \"\\n\\tBut, I exclaim, \" No, you won\\'t! \"\\n\\n\\tThis is only for that poor 37 million who have none.  Not for\\n\\tYOU, chum. :)  That comes LATER.\\n\\n\\tAdd in the estimates of the energy tax costs - $300-500 / year\\n\\n\\tPlus, all that extra \"corporate and rich\" taxes that will \\n\\ttrickle down, and what do you have?\\n\\n\\t$1,000 / year, just like I said two months ago.\\n\\n\\tAnd, the best part?   You don\\'t GET ANYTHING for it.\\n\\n\\tDeficit is STILL projected to rise at same rate it\\'s  been\\n\\trising at, by CLINTON\\'S OWN ESTIMATES.  And this assumes that\\n\\this plan WILL WORK!\\n\\n\\tI mean, come on, it doesn\\'t take a ROCKET SCIENTIST to see\\n\\tthat in another 2 or 3 years, we\\'re GETTING ANOTHER WHOPPING\\n\\tTAX INCREASE, because the deficit will STILL be GROWING \\n\\tFASTER THAN the ECONOMY.\\n\\n\\tAll Clinton is doing, is moving us to a HIGHER diving board.\\n\\n        Face it.  Clinton is Bush X 2.  In four more years, our\\n\\tcountry will be completely bankrupt, and your children\\'s\\n\\tfuture, so oft mentioned by Pal Bill, will be gone.\\n\\n\\tAnd those of you still deluding yourselves will be faced\\n\\twith the guilt.\\n\\n\\tWell, <glancing at watch>, gotta go.  I want to be out of\\n\\there by noon.  Got an appointment at the lake.  No tax\\n\\tthere, yet.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.iloc[3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sci.space\n",
       "1    sci.space\n",
       "2    sci.space\n",
       "3    sci.space\n",
       "4    sci.space\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** prepare data **********\n",
      "******** setup vector model **********\n",
      "******** model and XV **********\n",
      "Fold Accuracies: [ 0.53271028  0.5046729   0.5046729   0.56603774  0.51886792  0.48571429\n",
      "  0.4952381   0.4952381   0.56190476  0.51428571]\n",
      "XV Accuracy:  51.79%\n"
     ]
    }
   ],
   "source": [
    "print(\"******** prepare data **********\")\n",
    "new_data = []\n",
    "for i in range(text.shape[0]):\n",
    "    new_data.append(sub(\"[^a-zA-Z]\", \" \", text.iloc[i,0]))\n",
    "\n",
    "lowercase_data = []\n",
    "for i in range(len(new_data)):\n",
    "    lowercase_data.append(new_data[i].lower())\n",
    "\n",
    "stemmed_data = []\n",
    "for i in range(len(lowercase_data)):\n",
    "    words = lowercase_data[i].split()\n",
    "    stemmed_words = []\n",
    "    for w in words:\n",
    "        stemmed_words.append(stemmer.stem(w))\n",
    "    stemmed_data.append(\" \".join(stemmed_words))\n",
    "\n",
    "print(\"******** setup vector model **********\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True, min_df=2, stop_words='english')\n",
    "docarray = vectorizer.fit_transform(stemmed_data).toarray()\n",
    "\n",
    "print(\"******** model and XV **********\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# do the 10-fold cross validation\n",
    "scores = cross_val_score(model, docarray, label, cv=10)\n",
    "print(\"Fold Accuracies: {}\".format(scores))\n",
    "print(\"XV Accuracy: {: 6.2f}%\".format(scores.mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** setup **********\n",
      "******** prepare data **********\n",
      "******** setup vector model **********\n",
      "******** model and XV **********\n",
      "Fold Accuracies: [ 0.93457944  0.95327103  0.93457944  0.95283019  0.93396226  0.96190476\n",
      "  0.92380952  0.84761905  0.9047619   0.91428571]\n",
      "XV Accuracy:  92.62%\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from re import sub\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "print(\"******** setup **********\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# get the newsgroup database\n",
    "cats = ['talk.politics.misc', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=cats)\n",
    "\n",
    "# extract into dataframes\n",
    "text = pandas.DataFrame(newsgroups_train.data, columns=['text'])\n",
    "label = pandas.DataFrame(newsgroups_train.target, columns=['label'])['label'].apply(lambda x: cats[x])\n",
    "\n",
    "print(\"******** prepare data **********\")\n",
    "new_data = []\n",
    "for i in range(text.shape[0]):\n",
    "    new_data.append(sub(\"[^a-zA-Z]\", \" \", text.iloc[i,0]))\n",
    "\n",
    "lowercase_data = []\n",
    "for i in range(len(new_data)):\n",
    "    lowercase_data.append(new_data[i].lower())\n",
    "\n",
    "stemmed_data = []\n",
    "for i in range(len(lowercase_data)):\n",
    "    words = lowercase_data[i].split()\n",
    "    stemmed_words = []\n",
    "    for w in words:\n",
    "        stemmed_words.append(stemmer.stem(w))\n",
    "    stemmed_data.append(\" \".join(stemmed_words))\n",
    "\n",
    "print(\"******** setup vector model **********\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True, min_df=2, stop_words='english')\n",
    "docarray = vectorizer.fit_transform(stemmed_data).toarray()\n",
    "coords = vectorizer.get_feature_names()\n",
    "docterm = pandas.DataFrame(data=docarray,columns=coords)\n",
    "\n",
    "print(\"******** model and XV **********\")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "\n",
    "\n",
    "# do the 10-fold cross validation\n",
    "scores = cross_val_score(model, docarray, newsgroups_train.target, cv=10)\n",
    "print(\"Fold Accuracies: {}\".format(scores))\n",
    "print(\"XV Accuracy: {: 6.2f}%\".format(scores.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
